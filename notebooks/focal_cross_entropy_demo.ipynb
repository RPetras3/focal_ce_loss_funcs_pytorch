{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f530988",
   "metadata": {},
   "source": [
    "### Demo of Focal Cross-Entropy Loss Functions in PyTorch\n",
    "This module provides implementations of focal cross-entropy loss functions for both binary and multi-class classification tasks in PyTorch. The focal loss is designed to address class imbalance by down-weighting easy examples and focusing more on hard, misclassified examples.\n",
    "The `FocalCrossEntropyLoss` class extends the functionality of a standard cross-entropy loss by incorporating a focusing parameter (`gamma`) and an optional class balancing factor (`alpha`). It also includes support for label smoothing, which can help improve model generalization in multi-class classification scenarios.\n",
    "The `BinaryFocalCrossEntropyLoss` class is a specific implementation of the focal loss for binary classification tasks, while the `FocalCrossEntropyLoss` class is more general and can be used for multi-class classification tasks. Both classes allow for flexible configuration of the loss function to suit different training needs and scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb08ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install software dependencies for the focal cross-entropy loss functions demo notebook\n",
    "%pip install ipykernel requests matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b6e9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/__init__.py:2247\u001b[39m\n\u001b[32m   2231\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2232\u001b[39m \u001b[38;5;66;03m# Import most common subpackages\u001b[39;00m\n\u001b[32m   2233\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2238\u001b[39m \n\u001b[32m   2239\u001b[39m \u001b[38;5;66;03m# needs to be before import torch.nn as nn to avoid circular dependencies\u001b[39;00m\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2241\u001b[39m     enable_grad \u001b[38;5;28;01mas\u001b[39;00m enable_grad,\n\u001b[32m   2242\u001b[39m     inference_mode \u001b[38;5;28;01mas\u001b[39;00m inference_mode,\n\u001b[32m   2243\u001b[39m     no_grad \u001b[38;5;28;01mas\u001b[39;00m no_grad,\n\u001b[32m   2244\u001b[39m     set_grad_enabled \u001b[38;5;28;01mas\u001b[39;00m set_grad_enabled,\n\u001b[32m   2245\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m   2248\u001b[39m     __config__ \u001b[38;5;28;01mas\u001b[39;00m __config__,\n\u001b[32m   2249\u001b[39m     __future__ \u001b[38;5;28;01mas\u001b[39;00m __future__,\n\u001b[32m   2250\u001b[39m     _awaits \u001b[38;5;28;01mas\u001b[39;00m _awaits,\n\u001b[32m   2251\u001b[39m     accelerator \u001b[38;5;28;01mas\u001b[39;00m accelerator,\n\u001b[32m   2252\u001b[39m     autograd \u001b[38;5;28;01mas\u001b[39;00m autograd,\n\u001b[32m   2253\u001b[39m     backends \u001b[38;5;28;01mas\u001b[39;00m backends,\n\u001b[32m   2254\u001b[39m     cpu \u001b[38;5;28;01mas\u001b[39;00m cpu,\n\u001b[32m   2255\u001b[39m     cuda \u001b[38;5;28;01mas\u001b[39;00m cuda,\n\u001b[32m   2256\u001b[39m     distributed \u001b[38;5;28;01mas\u001b[39;00m distributed,\n\u001b[32m   2257\u001b[39m     distributions \u001b[38;5;28;01mas\u001b[39;00m distributions,\n\u001b[32m   2258\u001b[39m     fft \u001b[38;5;28;01mas\u001b[39;00m fft,\n\u001b[32m   2259\u001b[39m     futures \u001b[38;5;28;01mas\u001b[39;00m futures,\n\u001b[32m   2260\u001b[39m     hub \u001b[38;5;28;01mas\u001b[39;00m hub,\n\u001b[32m   2261\u001b[39m     jit \u001b[38;5;28;01mas\u001b[39;00m jit,\n\u001b[32m   2262\u001b[39m     linalg \u001b[38;5;28;01mas\u001b[39;00m linalg,\n\u001b[32m   2263\u001b[39m     mps \u001b[38;5;28;01mas\u001b[39;00m mps,\n\u001b[32m   2264\u001b[39m     mtia \u001b[38;5;28;01mas\u001b[39;00m mtia,\n\u001b[32m   2265\u001b[39m     multiprocessing \u001b[38;5;28;01mas\u001b[39;00m multiprocessing,\n\u001b[32m   2266\u001b[39m     nested \u001b[38;5;28;01mas\u001b[39;00m nested,\n\u001b[32m   2267\u001b[39m     nn \u001b[38;5;28;01mas\u001b[39;00m nn,\n\u001b[32m   2268\u001b[39m     optim \u001b[38;5;28;01mas\u001b[39;00m optim,\n\u001b[32m   2269\u001b[39m     overrides \u001b[38;5;28;01mas\u001b[39;00m overrides,\n\u001b[32m   2270\u001b[39m     profiler \u001b[38;5;28;01mas\u001b[39;00m profiler,\n\u001b[32m   2271\u001b[39m     sparse \u001b[38;5;28;01mas\u001b[39;00m sparse,\n\u001b[32m   2272\u001b[39m     special \u001b[38;5;28;01mas\u001b[39;00m special,\n\u001b[32m   2273\u001b[39m     testing \u001b[38;5;28;01mas\u001b[39;00m testing,\n\u001b[32m   2274\u001b[39m     types \u001b[38;5;28;01mas\u001b[39;00m types,\n\u001b[32m   2275\u001b[39m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[32m   2276\u001b[39m     version \u001b[38;5;28;01mas\u001b[39;00m version,\n\u001b[32m   2277\u001b[39m     xpu \u001b[38;5;28;01mas\u001b[39;00m xpu,\n\u001b[32m   2278\u001b[39m )\n\u001b[32m   2279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m windows \u001b[38;5;28;01mas\u001b[39;00m windows\n\u001b[32m   2282\u001b[39m \u001b[38;5;66;03m# Quantized, sparse, AO, etc. should be last to get imported, as nothing\u001b[39;00m\n\u001b[32m   2283\u001b[39m \u001b[38;5;66;03m# is expected to depend on them.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/nested/__init__.py:21\u001b[39m\n\u001b[32m     11\u001b[39m __all__ = [\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mto_padded_tensor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mas_nested_tensor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmasked_select\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m ]\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Allowlist these for weights_only load of NJT\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnested_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _rebuild_njt, NestedTensor \u001b[38;5;28;01mas\u001b[39;00m _NestedTensor\n\u001b[32m     24\u001b[39m torch.serialization.add_safe_globals([_NestedTensor, _rebuild_njt])\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mas_nested_tensor\u001b[39m(\n\u001b[32m     28\u001b[39m     ts: Tensor | \u001b[38;5;28mlist\u001b[39m[Tensor] | \u001b[38;5;28mtuple\u001b[39m[Tensor, ...],\n\u001b[32m     29\u001b[39m     dtype: DType | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     30\u001b[39m     device: Device | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     31\u001b[39m     layout=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     32\u001b[39m ) -> Tensor:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/nested/_internal/nested_tensor.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DispatchKey, DispatchKeySet\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_expandable_to\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnested\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnested_int\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NestedIntNode\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mweak\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WeakTensorKeyDictionary\n\u001b[32m     11\u001b[39m _tensor_id_counter = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/nested/_internal/nested_int.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_constant_symnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConstantIntNode\n\u001b[32m      7\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mNestedIntNode\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Python version of aten/src/ATen/core/NestedIntSymNodeImpl.cpp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/fx/__init__.py:88\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33mr\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mFX is a toolkit for developers to use to transform ``nn.Module``\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03minstances. FX consists of three main components: a **symbolic tracer,**\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03mrepository.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m immutable_collections\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_symbolic_trace\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     89\u001b[39m     PH,\n\u001b[32m     90\u001b[39m     ProxyableClassMeta,\n\u001b[32m     91\u001b[39m     symbolic_trace,\n\u001b[32m     92\u001b[39m     Tracer,\n\u001b[32m     93\u001b[39m     wrap,\n\u001b[32m     94\u001b[39m )\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CodeGen, Graph  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphModule\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/fx/_symbolic_trace.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_library\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopaque_object\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_opaque_reference_type, is_opaque_type\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _make_graph_module\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PyTreeCodeGen, _PyTreeInfo, Graph\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphModule\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/fx/_lazy_graph_module.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     _format_import_block,\n\u001b[32m      6\u001b[39m     GraphModule,\n\u001b[32m      7\u001b[39m     reduce_graph_module,\n\u001b[32m      8\u001b[39m     reduce_package_graph_module,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PackageExporter, sys_importer\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/fx/graph_module.py:24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _config \u001b[38;5;28;01mas\u001b[39;00m fx_experimental_config\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     _BoxedCodeGen,\n\u001b[32m     26\u001b[39m     _custom_builtins,\n\u001b[32m     27\u001b[39m     _is_from_torch,\n\u001b[32m     28\u001b[39m     _override_sym_repr,\n\u001b[32m     29\u001b[39m     _PyTreeCodeGen,\n\u001b[32m     30\u001b[39m     Graph,\n\u001b[32m     31\u001b[39m     PythonCode,\n\u001b[32m     32\u001b[39m )\n\u001b[32m     35\u001b[39m __all__ = [\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreduce_graph_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreduce_package_graph_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGraphModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m ]\n\u001b[32m     41\u001b[39m _USER_PRESERVED_ATTRIBUTES_KEY = \u001b[33m\"\u001b[39m\u001b[33m_user_preserved_attributes\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/focal_ce_loss_funcs_pytorch/.venv/lib64/python3.11/site-packages/torch/fx/graph.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fx_map_arg \u001b[38;5;28;01mas\u001b[39;00m map_arg, _NodeIter\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_library\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopaque_object\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_opaque_value_type\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dtype_abbrs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtype_abbrs\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pytree \u001b[38;5;28;01mas\u001b[39;00m fx_pytree\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1138\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1078\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1508\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1480\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1616\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# add src dir to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "# import the focal loss functions from the local file\n",
    "from focal_ce_loss import FocalCELoss\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682854b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the COCO dataset for the demo notebook\n",
    "# Download annotions and images for the COCO 2017 dataset\n",
    "dl_urls = [\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "           \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "           \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "           \"http://images.cocodataset.org/zips/test2017.zip\"]\n",
    "\n",
    "for url in dl_urls:\n",
    "    # create data/coco directory if it doesn't exist\n",
    "    os.makedirs(\"data/coco\", exist_ok=True)\n",
    "    # If the zips extracted directories already exist, skip downloading\n",
    "    if os.path.exists(f\"data/coco/{url.split('/')[-1].split('.')[0].split('_')[0]}\"):\n",
    "        print(f\"{url.split('/')[-1].split('.')[0].split('_')[0]} already exists, skipping download.\")\n",
    "        continue\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    response = requests.get(url)\n",
    "    with open(f\"data/coco/{filename}\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple cnn pytorch model to demo and compare the focal loss functions against other inbuilt loss functions\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(1024, 128)  # Assuming input images are 32x32\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bab48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the COCO dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
    "    transforms.ToTensor(),        # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Load the COCO dataset using torchvision\n",
    "train_dataset = datasets.CocoDetection(root=\"data/coco/train2017\",\n",
    "                                       annFile=\"data/coco/annotations/instances_train2017.json\",\n",
    "                                       transform=transform)\n",
    "val_dataset = datasets.CocoDetection(root=\"data/coco/val2017\",\n",
    "                                     annFile=\"data/coco/annotations/instances_val2017.json\",\n",
    "                                     transform=transform)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be00d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random sample images and their annotations from the training dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    img, annotations = train_dataset[i]\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(img.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "    ax.set_title(f\"Annotations: {len(annotations)}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the model and save the output metrics for comparison\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, annotations in train_loader:\n",
    "            # Assuming annotations contain the class labels for simplicity\n",
    "            labels = torch.tensor([ann[0]['category_id'] for ann in annotations])  # Extract class labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, annotations in val_loader:\n",
    "                labels = torch.tensor([ann[0]['category_id'] for ann in annotations])  # Extract class labels\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of criteria to compare the focal loss functions against\n",
    "critertia = {\"mse_loss\": nn.MSELoss(),\n",
    "             \"L1_loss\": nn.L1Loss(),\n",
    "             \"cross_entropy_loss\": nn.CrossEntropyLoss(),\n",
    "             \"nll_loss\": nn.NLLLoss(),\n",
    "             \"focal_ce_loss\": FocalCELoss(),\n",
    "             }\n",
    "# Train the model with each criterion and store the losses for comparison\n",
    "results = {}\n",
    "for name, criterion in critertia.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    model = SimpleCNN().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "    results[name] = {\"train_losses\": train_losses, \"val_losses\": val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses for each criterion\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, result in results.items():\n",
    "    plt.plot(result[\"train_losses\"], label=f\"{name} Train Loss\")\n",
    "    plt.plot(result[\"val_losses\"], label=f\"{name} Val Loss\")\n",
    "plt.title(\"Training and Validation Loss Comparison\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
