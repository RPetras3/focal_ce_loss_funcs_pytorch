{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f530988",
   "metadata": {},
   "source": [
    "### Demo of Focal Cross-Entropy Loss Functions in PyTorch\n",
    "This module provides implementations of focal cross-entropy loss functions for both binary and multi-class classification tasks in PyTorch. The focal loss is designed to address class imbalance by down-weighting easy examples and focusing more on hard, misclassified examples.\n",
    "The `FocalCrossEntropyLoss` class extends the functionality of a standard cross-entropy loss by incorporating a focusing parameter (`gamma`) and an optional class balancing factor (`alpha`). It also includes support for label smoothing, which can help improve model generalization in multi-class classification scenarios.\n",
    "The `BinaryFocalCrossEntropyLoss` class is a specific implementation of the focal loss for binary classification tasks, while the `FocalCrossEntropyLoss` class is more general and can be used for multi-class classification tasks. Both classes allow for flexible configuration of the loss function to suit different training needs and scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb08ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install software dependencies for the focal cross-entropy loss functions demo notebook\n",
    "%pip install ipykernel requests matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6e9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3013583679.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom focal_ce_loss-0.0.1 import FocalCELoss\u001b[39m\n                      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# import the focal loss functions from the local file\n",
    "from focal_ce_loss import FocalCELoss\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682854b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the COCO dataset for the demo notebook\n",
    "# Download annotions and images for the COCO 2017 dataset\n",
    "dl_urls = [\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "           \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "           \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "           \"http://images.cocodataset.org/zips/test2017.zip\"]\n",
    "\n",
    "for url in dl_urls:\n",
    "    # create data/coco directory if it doesn't exist\n",
    "    os.makedirs(\"data/coco\", exist_ok=True)\n",
    "    # If the zips extracted directories already exist, skip downloading\n",
    "    if os.path.exists(f\"data/coco/{url.split('/')[-1].split('.')[0].split('_')[0]}\"):\n",
    "        print(f\"{url.split('/')[-1].split('.')[0].split('_')[0]} already exists, skipping download.\")\n",
    "        continue\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    response = requests.get(url)\n",
    "    with open(f\"data/coco/{filename}\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple cnn pytorch model to demo and compare the focal loss functions against other inbuilt loss functions\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(1024, 128)  # Assuming input images are 32x32\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bab48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the COCO dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
    "    transforms.ToTensor(),        # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Load the COCO dataset using torchvision\n",
    "train_dataset = datasets.CocoDetection(root=\"data/coco/train2017\",\n",
    "                                       annFile=\"data/coco/annotations/instances_train2017.json\",\n",
    "                                       transform=transform)\n",
    "val_dataset = datasets.CocoDetection(root=\"data/coco/val2017\",\n",
    "                                     annFile=\"data/coco/annotations/instances_val2017.json\",\n",
    "                                     transform=transform)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be00d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random sample images and their annotations from the training dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    img, annotations = train_dataset[i]\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(img.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "    ax.set_title(f\"Annotations: {len(annotations)}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the model and save the output metrics for comparison\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, annotations in train_loader:\n",
    "            # Assuming annotations contain the class labels for simplicity\n",
    "            labels = torch.tensor([ann[0]['category_id'] for ann in annotations])  # Extract class labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, annotations in val_loader:\n",
    "                labels = torch.tensor([ann[0]['category_id'] for ann in annotations])  # Extract class labels\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of criteria to compare the focal loss functions against\n",
    "critertia = {\"mse_loss\": nn.MSELoss(),\n",
    "             \"L1_loss\": nn.L1Loss(),\n",
    "             \"cross_entropy_loss\": nn.CrossEntropyLoss(),\n",
    "             \"nll_loss\": nn.NLLLoss(),\n",
    "             \"focal_ce_loss\": FocalCELoss(),\n",
    "             }\n",
    "# Train the model with each criterion and store the losses for comparison\n",
    "results = {}\n",
    "for name, criterion in critertia.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    model = SimpleCNN().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "    results[name] = {\"train_losses\": train_losses, \"val_losses\": val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses for each criterion\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, result in results.items():\n",
    "    plt.plot(result[\"train_losses\"], label=f\"{name} Train Loss\")\n",
    "    plt.plot(result[\"val_losses\"], label=f\"{name} Val Loss\")\n",
    "plt.title(\"Training and Validation Loss Comparison\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
